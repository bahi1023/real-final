name: $(Date:yyyyMMdd)$(Rev:.r)-Addons
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]

variables:
  TF_DIR: '.' 
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_SERVICE_CONNECTION: 'bahi-aws-v2'
  AWS_REGION: 'us-east-1'

pool:
  vmImage: ubuntu-latest
 
#pool:
 # name: queue-bebo_mohammed

jobs:
- job: Addons
  displayName: 'Addons Deployment (Helm, Vault, Argo, Sonar, Nginx)'
  timeoutInMinutes: 160
  steps:
  - checkout: self

  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  - task: AWSShellScript@1
    displayName: Terraform init
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # Optimize: Enable Terraform Plugin Cache
        export TF_PLUGIN_CACHE_DIR="$HOME/.terraform.d/plugin-cache"
        mkdir -p "$TF_PLUGIN_CACHE_DIR"
        echo "Terraform Plugin Cache enabled at: $TF_PLUGIN_CACHE_DIR"
        
        # Install AWS CLI (Required because the agent environment lacks it, causing 'aws: command not found' errors)
        if ! command -v aws &> /dev/null; then
            echo "Installing AWS CLI..."
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install || ./aws/install -i ~/aws-cli -b ~/bin
            echo "##vso[task.prependpath]$HOME/bin"
        fi
        
        cd "$(TF_DIR)"
        terraform init

  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        CLUSTER_NAME="$(terraform output -raw cluster_name)"
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"

  # 2. Install Helm
  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  # 3. Install Nginx Ingress
  - task: AWSShellScript@1
    displayName: Install Nginx Ingress
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        # <--- THIS INSTALLS THE NGINX CONTROLLER WHICH CREATES THE AWS LOAD BALANCER (NLB)
        echo "Installing Nginx Ingress Controller..."
        helm upgrade --install ingress-nginx ingress-nginx \
          --repo https://kubernetes.github.io/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --set controller.service.type=LoadBalancer \
          --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
          --set controller.admissionWebhooks.enabled=false \
          --rollback-on-failure \
          --wait \
          --timeout 15m \
          --debug

  # 4. Fetch LB DNS and Update Terraform (Link APIGW)
  - task: AWSShellScript@1
    displayName: Link API Gateway to Load Balancer
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        
        echo "Fetching Load Balancer DNS..."
        NLB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "Found LB: $NLB_DNS"
        
        echo "Updating var.tf..."
        sed -i "s|https://example.com|https://$NLB_DNS|g" var.tf
        
        echo "Running Terraform Apply (Pass 2)..."
        terraform apply -auto-approve -var-file="$(TF_VAR_FILE)"

  # 5. Export Credentials and Terraform Outputs
  - task: AWSShellScript@1
    displayName: 'Export AWS Credentials & Terraform Outputs'
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -e
  
        cd "$(TF_DIR)"
  
        echo "Exporting AWS credentials to pipeline variables..."
  
        # Export AWS credentials to pipeline variables
        echo "##vso[task.setvariable variable=AWS_ACCESS_KEY_ID;isSecret=true]$AWS_ACCESS_KEY_ID"
        echo "##vso[task.setvariable variable=AWS_SECRET_ACCESS_KEY;isSecret=true]$AWS_SECRET_ACCESS_KEY"
  
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "##vso[task.setvariable variable=AWS_SESSION_TOKEN;isSecret=true]$AWS_SESSION_TOKEN"
        fi
  
        echo "Reading Terraform outputs..."
  
        # Safely read Terraform outputs
        # COGNITO_CLIENT_ID=$(terraform output -raw cognito_client_id || echo "")
        # COGNITO_CLIENT_SECRET=$(terraform output -raw cognito_client_secret || echo "")
        # COGNITO_ISSUER_URL=$(terraform output -raw cognito_issuer_url || echo "")
        CLUSTER_NAME=$(terraform output -raw cluster_name || echo "")
  
        # if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_CLIENT_SECRET" ]; then
        #   echo "##[error]Failed to read Cognito outputs from Terraform"
        #   terraform output
        #   exit 1
        # fi
  
        # Generate proper cookie secret (24 bytes base64 encoded = 32 chars)
        # COOKIE_SECRET=$(openssl rand -base64 24)
  
        # Set pipeline variables
        # echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        # echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        # echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        # echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"
  
        # Refresh kubeconfig
        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]Could not get cluster_name from Terraform output!"
          terraform output
          exit 1
        fi
        
        echo "Refreshing kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"
        
        # FIX: Persist AWS Credentials to disk for HelmDeploy
        # HelmDeploy task environment often strips AWS_* env vars from the aws-iam-authenticator.
        # Writing them to ~/.aws/credentials ensures the aws CLI always finds them.
        mkdir -p ~/.aws
        echo "Writing AWS credentials to ~/.aws/credentials..."
        
        cat > ~/.aws/credentials <<EOF
        [default]
        aws_access_key_id=$AWS_ACCESS_KEY_ID
        aws_secret_access_key=$AWS_SECRET_ACCESS_KEY
        EOF
        
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "aws_session_token=$AWS_SESSION_TOKEN" >> ~/.aws/credentials
        fi
        
        cat > ~/.aws/config <<EOF
        [default]
        region=$(AWS_REGION)
        EOF
        
        chmod 600 ~/.aws/credentials
        
        # Patch kubeconfig to use absolute aws path
        AWS_BIN=$(which aws)
        echo "Patching kubeconfig to use: $AWS_BIN"
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        # Export KUBECONFIG for subsequent tasks
        echo "##vso[task.setvariable variable=KUBECONFIG]$HOME/.kube/config"
  
        # Verify tools
        echo "AWS CLI Version:"
        aws --version
  
        echo "Kubeconfig (sanitized):"
        grep -v "data" ~/.kube/config || true
  
  # 6. Install OAuth2-Proxy
  # - task: AWSShellScript@1
  #   displayName: Install OAuth2-Proxy
  #   inputs:
  #     awsCredentials: $(AWS_SERVICE_CONNECTION)
  #     regionName: $(AWS_REGION)
  #     scriptType: inline
  #     inlineScript: |
  #       set -euo pipefail
        
  #       echo "Installing OAuth2-Proxy..."
        
  #       # Function to run on failure
  #       handle_error() {
  #         echo "##[error] OAuth2-Proxy deployment failed!"
  #         echo "Fetching debug logs..."
  #         kubectl get all -n ingress-nginx
  #         kubectl describe pod -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx || true
  #         kubectl logs -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx --all-containers --tail=100 || true
  #         kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' || true
  #       }
        
  #       # Set up trap to call handle_error on script failure
  #       trap 'handle_error' ERR

  #       helm upgrade --install oauth2-proxy oauth2-proxy \
  #         --repo https://oauth2-proxy.github.io/manifests \
  #         --namespace ingress-nginx --create-namespace \
  #         --wait --timeout 10m \
  #         --set config.clientID="$COGNITO_CLIENT_ID" \
  #         --set config.clientSecret="$COGNITO_CLIENT_SECRET" \
  #         --set config.cookieSecret="$COOKIE_SECRET" \
  #         --set extraArgs.provider="oidc" \
  #         --set extraArgs.oidc-issuer-url="$COGNITO_ISSUER_URL" \
  #         --set extraArgs.email-domain="gmail.com" \
  #         --set extraArgs.upstream="file:///dev/null" \
  #         --set extraArgs.http-address="0.0.0.0:4180" \
  #         # <--- THIS SECTION LINKS THE APP TO THE LOAD BALANCER (via Ingress Class "nginx")
  #         --set ingress.enabled=true \
  #         --set ingress.className="nginx" \
  #         --set ingress.path="/oauth2" \
  #         --set ingress.hosts[0]=""
  #   env:
  #     COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
  #     COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
  #     COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)
  #     COOKIE_SECRET: $(COOKIE_SECRET)

  # 7. Install Vault
  
  # FIX: Conflicting Webhook from previous installs causes upgrade failure. Force cleanup.
  - task: AWSShellScript@1
    displayName: Cleanup Vault Webhooks
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        echo "Cleaning up conflicting Vault webhooks..."
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true

  - task: HelmDeploy@0
    displayName: Install Vault
    inputs:
      connectionType: None
      command: upgrade
      chartType: Name
      chartName: vault
      releaseName: vault
      namespace: vault
      install: true
      arguments: >
        --repo https://helm.releases.hashicorp.com --create-namespace --wait --rollback-on-failure --timeout 5m
        --set server.service.type=ClusterIP
        # <--- THIS SECTION LINKS VAULT TO THE LOAD BALANCER
        --set server.ingress.enabled=true
        --set server.ingress.ingressClassName=nginx
        --set server.ingress.hosts[0].host=""
        --set "server.ingress.hosts[0].paths[0].path=/vault"
        # --set "server.ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-url=http://oauth2-proxy.ingress-nginx.svc.cluster.local:4180/oauth2/auth"
        # --set "server.ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-signin=https://$NLB_DNS/oauth2/start?rd=\$scheme://\$host\$request_uri"
    env:
      AWS_SHARED_CREDENTIALS_FILE: $(HOME)/.aws/credentials
      AWS_CONFIG_FILE: $(HOME)/.aws/config
      KUBECONFIG: $(KUBECONFIG)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
      AWS_REGION: $(AWS_REGION)

  # 8. Install Argo CD
  - task: HelmDeploy@0
    displayName: Install Argo CD
    inputs:
      connectionType: None
      command: upgrade
      chartType: Name
      chartName: argo-cd
      releaseName: argocd
      namespace: argocd
      install: true
      arguments: >
        --repo https://argoproj.github.io/argo-helm --create-namespace --wait --rollback-on-failure --timeout 5m
        --set server.service.type=ClusterIP
        # <--- THIS SECTION LINKS ARGOCD TO THE LOAD BALANCER
        --set server.ingress.enabled=true
        --set server.ingress.ingressClassName=nginx
        --set "server.ingress.paths[0]=/argoCD"
        --set "server.extraArgs={--insecure,--rootpath=/argoCD}"
        # --set "server.ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-url=http://oauth2-proxy.ingress-nginx.svc.cluster.local:4180/oauth2/auth"
        # --set "server.ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-signin=https://$NLB_DNS/oauth2/start?rd=\$scheme://\$host\$request_uri"
    env:
      AWS_SHARED_CREDENTIALS_FILE: $(HOME)/.aws/credentials
      AWS_CONFIG_FILE: $(HOME)/.aws/config
      KUBECONFIG: $(KUBECONFIG)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
      AWS_REGION: $(AWS_REGION)

  # 9. Install SonarQube
  - task: HelmDeploy@0
    displayName: Install SonarQube
    inputs:
      connectionType: None
      command: upgrade
      chartType: Name
      chartName: sonarqube
      releaseName: sonarqube
      namespace: sonarqube
      install: true
      arguments: >
        --repo https://SonarSource.github.io/helm-chart-sonarqube --create-namespace --wait --rollback-on-failure --timeout 10m
        --set monitoringPasscode=changeMe
        --set community.enabled=true
        --set edition=
        --set service.type=ClusterIP
        # <--- THIS SECTION LINKS SONARQUBE TO THE LOAD BALANCER
        --set ingress.enabled=true
        --set ingress.ingressClassName=nginx
        --set ingress.hosts[0].name=""
        --set ingress.hosts[0].path=/sonarqube
        --set "sonarProperties.sonar\.web\.context=/sonarqube"
        # --set "ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-url=http://oauth2-proxy.ingress-nginx.svc.cluster.local:4180/oauth2/auth"
        # --set "ingress.annotations.nginx\.ingress\.kubernetes\.io/auth-signin=https://$NLB_DNS/oauth2/start?rd=\$scheme://\$host\$request_uri"
    env:
      AWS_SHARED_CREDENTIALS_FILE: $(HOME)/.aws/credentials
      AWS_CONFIG_FILE: $(HOME)/.aws/config
      KUBECONFIG: $(KUBECONFIG)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
      AWS_REGION: $(AWS_REGION)
