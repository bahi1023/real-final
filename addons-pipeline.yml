name: $(Date:yyyyMMdd)$(Rev:.r)-Addons
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    values: [nonprod, prod]

variables:
  TF_DIR: '.' 
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_SERVICE_CONNECTION: 'bahi-aws-v2'
  AWS_REGION: 'us-east-1'

# pool:
#   vmImage: ubuntu-latest
 
pool:
 name: queue-bebo_mohammed

jobs:
- job: Addons
  displayName: 'Addons Deployment (Helm, Vault, Argo, Sonar, Nginx)'
  timeoutInMinutes: 160
  steps:
  - checkout: self

  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  - task: AWSShellScript@1
    displayName: Terraform init
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # Optimize: Enable Terraform Plugin Cache
        export TF_PLUGIN_CACHE_DIR="$HOME/.terraform.d/plugin-cache"
        mkdir -p "$TF_PLUGIN_CACHE_DIR"
        echo "Terraform Plugin Cache enabled at: $TF_PLUGIN_CACHE_DIR"
        
        # Install AWS CLI (Required because the agent environment lacks it, causing 'aws: command not found' errors)
        if ! command -v aws &> /dev/null; then
            echo "Installing AWS CLI..."
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install || ./aws/install -i ~/aws-cli -b ~/bin
            echo "##vso[task.prependpath]$HOME/bin"
        fi
        
        cd "$(TF_DIR)"
        terraform init

  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        
        # Safely fetch cluster name
        CLUSTER_NAME=$(terraform output -raw cluster_name || echo "")
        
        if [ -z "$CLUSTER_NAME" ]; then
           echo "##[warning]Could not get cluster_name. Skipping kubeconfig setup."
           exit 0
        fi
        
        echo "Updating kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"

  # 2. Install Helm
  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  # 3. Install Nginx Ingress
  - task: AWSShellScript@1
    displayName: Install Nginx Ingress
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # Function to run on failure
        handle_error() {
          echo "##[error] Nginx Ingress deployment failed!"
          echo "Fetching debug logs..."
          kubectl get all -n ingress-nginx
          kubectl describe pod -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx || true
          kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx --all-containers --tail=100 || true
          kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' || true
        }
        
        # Set up trap to call handle_error on script failure
        trap 'handle_error' ERR

        # <--- THIS INSTALLS THE NGINX CONTROLLER WHICH CREATES THE AWS LOAD BALANCER (NLB)
        echo "Installing Nginx Ingress Controller..."
        helm upgrade --install ingress-nginx ingress-nginx \
          --repo https://kubernetes.github.io/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --version 4.11.3 \
          --set controller.service.type=LoadBalancer \
          --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
          --set controller.admissionWebhooks.enabled=false \
          --wait \
          --timeout 15m \
          --debug

  # 4. Fetch LB DNS and Update Terraform (Link APIGW)
  - task: AWSShellScript@1
    displayName: Link API Gateway to Load Balancer
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        
        echo "Fetching Load Balancer DNS..."
        NLB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "")
        
        if [ -z "$NLB_DNS" ]; then
          echo "##[warning]Load Balancer DNS not found yet. Skipping API Gateway update."
        else
          echo "Found LB: $NLB_DNS"
          
          # Extract NLB Name from DNS (first part before the first dash)
          NLB_NAME=$(echo $NLB_DNS | cut -d'-' -f1)
          
          echo "Fetching LB ARN for $NLB_NAME..."
          LB_ARN=$(aws elbv2 describe-load-balancers --names "$NLB_NAME" --query 'LoadBalancers[0].LoadBalancerArn' --output text)
          
          echo "Fetching Listener ARN..."
          # Fetch the first listener (usually port 80 for Nginx ingress)
          LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[0].ListenerArn' --output text)
          
          echo "Found Listener ARN: $LISTENER_ARN"
          echo "Running Terraform Apply to sync API Gateway..."
          terraform apply -auto-approve -var-file="$(TF_VAR_FILE)" \
            -var="nlb_dns_name=http://$NLB_DNS" \
            -var="nlb_listener_arn=$LISTENER_ARN"
        fi

  # 5. Export Credentials and Terraform Outputs
  - task: AWSShellScript@1
    displayName: 'Export AWS Credentials & Terraform Outputs'
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -e
  
        cd "$(TF_DIR)"
  
        echo "Exporting AWS credentials to pipeline variables..."
  
        # Export AWS credentials to pipeline variables
        echo "##vso[task.setvariable variable=AWS_ACCESS_KEY_ID;isSecret=true]$AWS_ACCESS_KEY_ID"
        echo "##vso[task.setvariable variable=AWS_SECRET_ACCESS_KEY;isSecret=true]$AWS_SECRET_ACCESS_KEY"
  
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "##vso[task.setvariable variable=AWS_SESSION_TOKEN;isSecret=true]$AWS_SESSION_TOKEN"
        fi
  
        echo "Reading Terraform outputs..."
  
        # Safely read Terraform outputs
        COGNITO_CLIENT_ID=$(terraform output -raw cognito_client_id || echo "")
        COGNITO_CLIENT_SECRET=$(terraform output -raw cognito_client_secret || echo "")
        COGNITO_ISSUER_URL=$(terraform output -raw cognito_issuer_url || echo "")
        CLUSTER_NAME=$(terraform output -raw cluster_name || echo "")
  
        if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_CLIENT_SECRET" ]; then
          echo "##[error]Failed to read Cognito outputs from Terraform"
          terraform output
          exit 1
        fi
  
        # COOKIE_SECRET must be exactly 32 bytes for AES-256. 
        # This one is exactly 32 bytes: "STABLE_SECRET_V1_FOR_32_BYTE_AES"
        COOKIE_SECRET="STABLE_SECRET_V1_FOR_32_BYTE_AES"
  
        # Set pipeline variables
        echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"
   
        # Refresh kubeconfig
        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]Could not get cluster_name from Terraform output!"
          terraform output
          exit 1
        fi
        
        echo "Refreshing kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"
        
        # FIX: Persist AWS Credentials to disk for HelmDeploy
        # HelmDeploy task environment often strips AWS_* env vars from the aws-iam-authenticator.
        # Writing them to ~/.aws/credentials ensures the aws CLI always finds them.
        mkdir -p ~/.aws
        echo "Writing AWS credentials to ~/.aws/credentials..."
        
        cat > ~/.aws/credentials <<EOF
        [default]
        aws_access_key_id=$AWS_ACCESS_KEY_ID
        aws_secret_access_key=$AWS_SECRET_ACCESS_KEY
        EOF
        
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "aws_session_token=$AWS_SESSION_TOKEN" >> ~/.aws/credentials
        fi
        
        cat > ~/.aws/config <<EOF
        [default]
        region=$(AWS_REGION)
        EOF
        
        chmod 600 ~/.aws/credentials
        
        # Patch kubeconfig to use absolute aws path
        AWS_BIN=$(which aws)
        echo "Patching kubeconfig to use: $AWS_BIN"
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        # Export KUBECONFIG for subsequent tasks
        echo "##vso[task.setvariable variable=KUBECONFIG]$HOME/.kube/config"
  
        # Verify tools
        echo "AWS CLI Version:"
        aws --version
  
        echo "Kubeconfig (sanitized):"
        grep -v "data" ~/.kube/config || true
  
  # 6. Install OAuth2-Proxy
  - task: AWSShellScript@1
    displayName: I  
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        echo "Installing OAuth2-Proxy..."
        
        # Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        APIGW_URL=$(terraform output -raw api_gateway_url)
        
        # Function to run on failure
        handle_error() {
          echo "##[error] OAuth2-Proxy deployment failed!"
          echo "Fetching debug logs..."
          kubectl get all -n ingress-nginx
          kubectl describe pod -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx || true
          kubectl logs -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx --all-containers --tail=100 || true
          kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' || true
        }
        
        # Set up trap to call handle_error on script failure
        trap 'handle_error' ERR

        # FIX: Check for stuck Helm releases (e.g., pending-upgrade) and clear them
        STATUS=$(helm status oauth2-proxy -n ingress-nginx -o json | jq -r .info.status || echo "not-found")
        if [[ "$STATUS" == "pending-upgrade" || "$STATUS" == "pending-install" ]]; then
          echo "##[warning] Found stuck Helm release (status: $STATUS). Clearing it..."
          helm rollback oauth2-proxy -n ingress-nginx || helm uninstall oauth2-proxy -n ingress-nginx || true
        fi

        # This installs OAuth2-Proxy and links it to the Load Balancer via Ingress Class "nginx"
        helm upgrade --install oauth2-proxy oauth2-proxy \
          --repo https://oauth2-proxy.github.io/manifests \
          --namespace ingress-nginx --create-namespace \
          --wait --timeout 15m --atomic \
          --set config.clientID="$COGNITO_CLIENT_ID" \
          --set config.clientSecret="$COGNITO_CLIENT_SECRET" \
          --set config.cookieSecret="$COOKIE_SECRET" \
          --set extraArgs.provider="oidc" \
          --set extraArgs.oidc-issuer-url="$COGNITO_ISSUER_URL" \
          --set extraArgs.email-domain="gmail.com" \
          --set extraArgs.skip-provider-button=true \
          --set extraArgs.upstream="file:///dev/null" \
          --set extraArgs.http-address="0.0.0.0:4180" \
          --set extraArgs.redirect-url="${APIGW_URL}/oauth2/callback" \
          --set extraArgs.session-cookie-minimal=true \
          --set extraArgs.pass-access-token=false \
          --set ingress.enabled=true \
          --set ingress.className="nginx" \
          --set ingress.path="/oauth2" \
          --set ingress.hosts[0]=""
    env:
      COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
      COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
      COOKIE_SECRET: $(COOKIE_SECRET)
      COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)

  # 7. Install Vault
  
  # FIX: Conflicting Webhook from previous installs causes upgrade failure. Force cleanup.
  - task: AWSShellScript@1
    displayName: Cleanup Vault Webhooks
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        echo "Cleaning up conflicting Vault webhooks..."
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true

  - task: AWSShellScript@1
    displayName: Install Vault
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
        
        # 3. Patch Kubeconfig to use absolute path (Security/Path fix)
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true

        # 3. Patch Kubeconfig
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true

        echo "Installing Vault..."
        
        # Create values.yaml to avoid quoting issues with --set (like SonarQube)
        cat > vault-ingress-values.yaml <<EOF
        server:
          ha:
            enabled: false
            replicas: 1
          dataStorage:
            storageClass: gp2
            size: 1Gi
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: ""
                paths:
                  - /vault/?(.*)
                  - /(ui(?:/.*)?)
            annotations:
              nginx.ingress.kubernetes.io/use-regex: "true"
              nginx.ingress.kubernetes.io/rewrite-target: /\$1
        ui:
          enabled: true
        EOF
        
        # Delete existing Vault StatefulSet if needed (to allow storage changes)
        # kubectl delete statefulset vault -n bahi --ignore-not-found=true

        helm upgrade --install vault vault \
          --repo https://helm.releases.hashicorp.com \
          --namespace bahi --create-namespace \
          --wait --timeout 5m \
          -f vault-ingress-values.yaml

  # 8. Install Argo CD
  - task: AWSShellScript@1
    displayName: Install Argo CD
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
         
        # 3. Patch Kubeconfig
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        echo "Installing Argo CD..."
        
        # Create values.yaml to avoid quoting issues with --set. Strip 8 leading spaces to ensure valid YAML.
        cat <<'EOF' | sed 's/^        //' > argocd-ingress-values.yaml
        redis-ha:
          enabled: false
        controller:
          replicas: 1
        server:
          replicas: 1
          service:
            type: ClusterIP
          ingress:
            enabled: false
          extraArgs:
            - --insecure
            - --rootpath=/argocd
        repoServer:
          replicas: 1
        applicationController:
          replicas: 1
        EOF
        
        # Cleanup potential stuck jobs from previous failed installs
        # kubectl delete job -n bahi argocd-redis-secret-init --ignore-not-found=true
        
        n=0
        until [ "$n" -ge 3 ]
        do
           helm upgrade --install argocd argo-cd \
             --repo https://argoproj.github.io/argo-helm \
             --namespace bahi --create-namespace \
             --wait --timeout 10m \
             -f argocd-ingress-values.yaml && break
           n=$((n+1))
           echo "Deploy failed (attempt $n/3). Retrying in 10s..."
           # Aggressive cleanup: Uninstall to clear bad state/conflicts
           helm uninstall argocd -n bahi || true
           # Wait a bit for resources to terminate
           sleep 20
        done
        
        if [ "$n" -eq 3 ]; then
           echo "ArgoCD helm upgrade failed after 3 attempts."
           exit 1
        fi
        
        echo "Applying standalone ArgoCD Ingress..."
        kubectl apply -f - <<'INGRESS_EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: argocd-server
          namespace: bahi
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: HTTP
        spec:
          ingressClassName: nginx
          rules:
          - http:
              paths:
              - path: /argocd
                pathType: Prefix
                backend:
                  service:
                    name: argocd-server
                    port:
                      number: 80
        INGRESS_EOF
        
        echo "Waiting for ArgoCD Redis Secret Init Job..."
        # Wait up to 5 minutes for the job to complete
        # if ! kubectl wait --for=condition=complete job/argocd-redis-secret-init -n bahi --timeout=300s; then
        #     echo "ArgoCD Redis Secret Init Job timed out or failed. Debugging..."
        #     kubectl get pods -n bahi
        #     kubectl describe job -n bahi argocd-redis-secret-init || true
        #     kubectl logs -n bahi -l job-name=argocd-redis-secret-init --tail=100 || true
        #     # Check for events in the namespace to see pulling errors
        #     kubectl get events -n bahi --sort-by='.lastTimestamp' | tail -n 20
        #     exit 1
        # fi
        
        echo "ArgoCD Redis Secret Init Job completed successfully."

  # 9. Install SonarQube
  - task: AWSShellScript@1
    displayName: Install SonarQube
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
        
        # 3. Patch Kubeconfig
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        echo "Installing SonarQube..."
        
        # Create values.yaml to avoid quoting issues with --set
        cat > sonarqube-values.yaml <<EOF
        monitoringPasscode: changeMe
        community:
          enabled: true
        service:
          type: ClusterIP
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - name: ""
              path: "/sonarqube"
          annotations:
             nginx.ingress.kubernetes.io/proxy-body-size: "64m"
        sonarWebContext: /sonarqube
        EOF

        if ! helm upgrade --install sonarqube sonarqube \
          --repo https://SonarSource.github.io/helm-chart-sonarqube \
          --namespace bahi --create-namespace \
          --wait --timeout 10m \
          -f sonarqube-values.yaml; then
            echo "SonarQube installation failed. Debugging..."
            kubectl get pods -n bahi -l app=sonarqube
            kubectl describe pods -n bahi -l app=sonarqube
            kubectl logs -n bahi -l app=sonarqube --tail=100
            exit 1
        fi