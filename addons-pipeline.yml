name: $(Date:yyyyMMdd)$(Rev:.r)-Addons
trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]

variables:
  TF_DIR: '.' 
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_SERVICE_CONNECTION: 'bahi-aws-v2'
  AWS_REGION: 'us-east-1'

# pool:
#   vmImage: ubuntu-latest
 
pool:
 name: queue-bebo_mohammed

jobs:
- job: Addons
  displayName: 'Addons Deployment (Helm, Vault, Argo, Sonar, Nginx)'
  timeoutInMinutes: 160
  steps:
  - checkout: self

  - task: TerraformInstaller@1
    displayName: Install Terraform
    inputs:
      terraformVersion: latest

  - task: AWSShellScript@1
    displayName: Terraform init
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # Optimize: Enable Terraform Plugin Cache
        export TF_PLUGIN_CACHE_DIR="$HOME/.terraform.d/plugin-cache"
        mkdir -p "$TF_PLUGIN_CACHE_DIR"
        echo "Terraform Plugin Cache enabled at: $TF_PLUGIN_CACHE_DIR"
        
        # Install AWS CLI (Required because the agent environment lacks it, causing 'aws: command not found' errors)
        if ! command -v aws &> /dev/null; then
            echo "Installing AWS CLI..."
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install || ./aws/install -i ~/aws-cli -b ~/bin
            echo "##vso[task.prependpath]$HOME/bin"
        fi
        
        cd "$(TF_DIR)"
        terraform init

  - task: AWSShellScript@1
    displayName: Configure kubeconfig
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        CLUSTER_NAME="$(terraform output -raw cluster_name)"
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"

  # 2. Install Helm
  - task: HelmInstaller@1
    displayName: Install Helm
    inputs:
      helmVersionToInstall: 'latest'

  # 3. Install Nginx Ingress
  - task: AWSShellScript@1
    displayName: Install Nginx Ingress
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # Function to run on failure
        handle_error() {
          echo "##[error] Nginx Ingress deployment failed!"
          echo "Fetching debug logs..."
          kubectl get all -n ingress-nginx
          kubectl describe pod -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx || true
          kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx --all-containers --tail=100 || true
          kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' || true
        }
        
        # Set up trap to call handle_error on script failure
        trap 'handle_error' ERR

        # <--- THIS INSTALLS THE NGINX CONTROLLER WHICH CREATES THE AWS LOAD BALANCER (NLB)
        echo "Installing Nginx Ingress Controller..."
        helm upgrade --install ingress-nginx ingress-nginx \
          --repo https://kubernetes.github.io/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --version 4.11.3 \
          --set controller.service.type=LoadBalancer \
          --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="nlb" \
          --set controller.admissionWebhooks.enabled=false \
          --wait \
          --timeout 15m \
          --debug

  # 4. Fetch LB DNS and Update Terraform (Link APIGW)
  - task: AWSShellScript@1
    displayName: Link API Gateway to Load Balancer
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        cd "$(TF_DIR)"
        
        echo "Fetching Load Balancer DNS..."
        NLB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "Found LB: $NLB_DNS"
        
        echo "Updating var.tf..."
        sed -i "s|https://example.com|https://$NLB_DNS|g" var.tf
        
        echo "Running Terraform Apply (Pass 2)..."
        terraform apply -auto-approve -var-file="$(TF_VAR_FILE)"

  # 5. Export Credentials and Terraform Outputs
  - task: AWSShellScript@1
    displayName: 'Export AWS Credentials & Terraform Outputs'
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -e
  
        cd "$(TF_DIR)"
  
        echo "Exporting AWS credentials to pipeline variables..."
  
        # Export AWS credentials to pipeline variables
        echo "##vso[task.setvariable variable=AWS_ACCESS_KEY_ID;isSecret=true]$AWS_ACCESS_KEY_ID"
        echo "##vso[task.setvariable variable=AWS_SECRET_ACCESS_KEY;isSecret=true]$AWS_SECRET_ACCESS_KEY"
  
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "##vso[task.setvariable variable=AWS_SESSION_TOKEN;isSecret=true]$AWS_SESSION_TOKEN"
        fi
  
        echo "Reading Terraform outputs..."
  
        # Safely read Terraform outputs
        # COGNITO_CLIENT_ID=$(terraform output -raw cognito_client_id || echo "")
        # COGNITO_CLIENT_SECRET=$(terraform output -raw cognito_client_secret || echo "")
        # COGNITO_ISSUER_URL=$(terraform output -raw cognito_issuer_url || echo "")
        CLUSTER_NAME=$(terraform output -raw cluster_name || echo "")
  
        # if [ -z "$COGNITO_CLIENT_ID" ] || [ -z "$COGNITO_CLIENT_SECRET" ]; then
        #   echo "##[error]Failed to read Cognito outputs from Terraform"
        #   terraform output
        #   exit 1
        # fi
  
        # Generate proper cookie secret (24 bytes base64 encoded = 32 chars)
        # COOKIE_SECRET=$(openssl rand -base64 24)
  
        # Set pipeline variables
        # echo "##vso[task.setvariable variable=COGNITO_CLIENT_ID;isSecret=true]$COGNITO_CLIENT_ID"
        # echo "##vso[task.setvariable variable=COGNITO_CLIENT_SECRET;isSecret=true]$COGNITO_CLIENT_SECRET"
        # echo "##vso[task.setvariable variable=COGNITO_ISSUER_URL]$COGNITO_ISSUER_URL"
        # echo "##vso[task.setvariable variable=COOKIE_SECRET;isSecret=true]$COOKIE_SECRET"
  
        # Refresh kubeconfig
        if [ -z "$CLUSTER_NAME" ]; then
          echo "##[error]Could not get cluster_name from Terraform output!"
          terraform output
          exit 1
        fi
        
        echo "Refreshing kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$(AWS_REGION)"
        
        # FIX: Persist AWS Credentials to disk for HelmDeploy
        # HelmDeploy task environment often strips AWS_* env vars from the aws-iam-authenticator.
        # Writing them to ~/.aws/credentials ensures the aws CLI always finds them.
        mkdir -p ~/.aws
        echo "Writing AWS credentials to ~/.aws/credentials..."
        
        cat > ~/.aws/credentials <<EOF
        [default]
        aws_access_key_id=$AWS_ACCESS_KEY_ID
        aws_secret_access_key=$AWS_SECRET_ACCESS_KEY
        EOF
        
        if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
          echo "aws_session_token=$AWS_SESSION_TOKEN" >> ~/.aws/credentials
        fi
        
        cat > ~/.aws/config <<EOF
        [default]
        region=$(AWS_REGION)
        EOF
        
        chmod 600 ~/.aws/credentials
        
        # Patch kubeconfig to use absolute aws path
        AWS_BIN=$(which aws)
        echo "Patching kubeconfig to use: $AWS_BIN"
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        # Export KUBECONFIG for subsequent tasks
        echo "##vso[task.setvariable variable=KUBECONFIG]$HOME/.kube/config"
  
        # Verify tools
        echo "AWS CLI Version:"
        aws --version
  
        echo "Kubeconfig (sanitized):"
        grep -v "data" ~/.kube/config || true
  
  # 6. Install OAuth2-Proxy
  # - task: AWSShellScript@1
  #   displayName: Install OAuth2-Proxy
  #   inputs:
  #     awsCredentials: $(AWS_SERVICE_CONNECTION)
  #     regionName: $(AWS_REGION)
  #     scriptType: inline
  #     inlineScript: |
  #       set -euo pipefail
        
  #       echo "Installing OAuth2-Proxy..."
        
  #       # Function to run on failure
  #       handle_error() {
  #         echo "##[error] OAuth2-Proxy deployment failed!"
  #         echo "Fetching debug logs..."
  #         kubectl get all -n ingress-nginx
  #         kubectl describe pod -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx || true
  #         kubectl logs -l app.kubernetes.io/name=oauth2-proxy -n ingress-nginx --all-containers --tail=100 || true
  #         kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' || true
  #       }
        
  #       # Set up trap to call handle_error on script failure
  #       trap 'handle_error' ERR

  #       helm upgrade --install oauth2-proxy oauth2-proxy \
  #         --repo https://oauth2-proxy.github.io/manifests \
  #         --namespace ingress-nginx --create-namespace \
  #         --wait --timeout 10m \
  #         --set config.clientID="$COGNITO_CLIENT_ID" \
  #         --set config.clientSecret="$COGNITO_CLIENT_SECRET" \
  #         --set config.cookieSecret="$COOKIE_SECRET" \
  #         --set extraArgs.provider="oidc" \
  #         --set extraArgs.oidc-issuer-url="$COGNITO_ISSUER_URL" \
  #         --set extraArgs.email-domain="gmail.com" \
  #         --set extraArgs.upstream="file:///dev/null" \
  #         --set extraArgs.http-address="0.0.0.0:4180" \
  #         # <--- THIS SECTION LINKS THE APP TO THE LOAD BALANCER (via Ingress Class "nginx")
  #         --set ingress.enabled=true \
  #         --set ingress.className="nginx" \
  #         --set ingress.path="/oauth2" \
  #         --set ingress.hosts[0]=""
  #   env:
  #     COGNITO_CLIENT_ID: $(COGNITO_CLIENT_ID)
  #     COGNITO_CLIENT_SECRET: $(COGNITO_CLIENT_SECRET)
  #     COGNITO_ISSUER_URL: $(COGNITO_ISSUER_URL)
  #     COOKIE_SECRET: $(COOKIE_SECRET)

  # 7. Install Vault
  
  # FIX: Conflicting Webhook from previous installs causes upgrade failure. Force cleanup.
  - task: AWSShellScript@1
    displayName: Cleanup Vault Webhooks
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        echo "Cleaning up conflicting Vault webhooks..."
        kubectl delete MutatingWebhookConfiguration vault-agent-injector-cfg --ignore-not-found=true

  - task: AWSShellScript@1
    displayName: Install Vault
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
        
        # 3. Patch Kubeconfig to use absolute path (Security/Path fix)
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true

        echo "Cleaning up potential old Vault resources..."
        kubectl delete clusterrole vault-agent-injector-clusterrole --ignore-not-found=true
        kubectl delete clusterrolebinding vault-agent-injector-binding --ignore-not-found=true
        kubectl delete clusterrolebinding vault-server-binding --ignore-not-found=true
        
        echo "Installing Vault..."
        
        # Create values.yaml to avoid quoting issues with --set (like SonarQube)
        cat > vault-ingress-values.yaml <<EOF
        server:
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: ""
                paths:
                  - /vault
            annotations:
              nginx.ingress.kubernetes.io/rewrite-target: /
        ui:
          enabled: true
        EOF
        
        helm upgrade --install vault vault \
          --repo https://helm.releases.hashicorp.com \
          --namespace default\
          --wait --timeout 5m \
          -f vault-ingress-values.yaml

  # 8. Install Argo CD
  - task: AWSShellScript@1
    displayName: Install Argo CD
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
         
        # 3. Patch Kubeconfig
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        echo "Installing Argo CD..."
        
        # Create values.yaml to avoid quoting issues with --set (like SonarQube)
        cat > argocd-ingress-values.yaml <<EOF
        server:
          service:
            type: ClusterIP
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - ""
            paths:
              - /argocd
            annotations:
              nginx.ingress.kubernetes.io/rewrite-target: /
              nginx.ingress.kubernetes.io/backend-protocol: HTTPS
          extraArgs:
            - --insecure
            - --rootpath=/argocd
        EOF
        
        helm upgrade --install argocd argo-cd \
          --repo https://argoproj.github.io/argo-helm \
          --namespace default \
          --wait --timeout 5m \
          -f argocd-ingress-values.yaml

  # 9. Install SonarQube
  - task: AWSShellScript@1
    displayName: Install SonarQube
    inputs:
      awsCredentials: $(AWS_SERVICE_CONNECTION)
      regionName: $(AWS_REGION)
      scriptType: inline
      inlineScript: |
        set -euo pipefail
        
        # 1. Go to Terraform dir to get outputs
        cd "$(TF_DIR)"
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        
        # 2. Update Kubeconfig
        echo "Updating Kubeconfig for $CLUSTER_NAME..."
        aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $(AWS_REGION)
        
        # 3. Patch Kubeconfig
        AWS_BIN=$(which aws)
        sed -i "s|command: aws|command: $AWS_BIN|g" ~/.kube/config || true
        
        echo "Installing SonarQube..."
        
        # Create values.yaml to avoid quoting issues with --set
        cat > sonarqube-values.yaml <<EOF
        monitoringPasscode: changeMe
        community:
          enabled: true
        service:
          type: ClusterIP
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - name: ""
              path: "/sonarqube"
          annotations:
             nginx.ingress.kubernetes.io/proxy-body-size: "64m"
        sonarWebContext: /sonarqube
        EOF

        helm upgrade --install sonarqube sonarqube \
          --repo https://SonarSource.github.io/helm-chart-sonarqube \
          --namespace default \
          --wait --timeout 10m \
          -f sonarqube-values.yaml